{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by creating a simple Neural Network we can use to explore activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Neural Network\n",
    "def preprocess_data(x_train, y_train, x_test, y_test, num_classes=10):\n",
    "    x_train = x_train.reshape(-1, 784) / 255.0\n",
    "    x_test = x_test.reshape(-1, 784) / 255.0\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "class MLPModel:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=self.input_shape),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=5, batch_size=128):\n",
    "        self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implament a function to get the activations from our neural network and get some cool visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronActivationComputer:\n",
    "    def __init__(self, model, x_test, y_test):\n",
    "        self.model = model\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def compute_activations(self):\n",
    "        num_classes = self.y_test.shape[-1]\n",
    "\n",
    "        # Get the layer outputs for the test data\n",
    "        layer_outputs = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'output'):\n",
    "                layer_model = tf.keras.Model(inputs=self.model.input, outputs=layer.output)\n",
    "                layer_outputs.append(layer_model.predict(self.x_test))\n",
    "\n",
    "        # Compute the neuron activations averaged for each label and each layer\n",
    "        layer_activations = []\n",
    "        for outputs in layer_outputs:\n",
    "            num_neurons_in_layer = outputs.shape[-1]\n",
    "            layer_activation = np.zeros((num_classes, num_neurons_in_layer))\n",
    "            for label_idx in range(num_classes):\n",
    "                layer_activation[label_idx] = np.mean(outputs[np.argmax(self.y_test, axis=1) == label_idx], axis=0)\n",
    "            layer_activations.append(layer_activation)\n",
    "\n",
    "        return layer_activations\n",
    "\n",
    "    def visualize_activations(self, layer_idx, class_idx=None):\n",
    "        layer_activations = self.compute_activations()\n",
    "        activations = layer_activations[layer_idx]\n",
    "\n",
    "        if class_idx is None:\n",
    "            # Visualize activations for all classes\n",
    "            fig, axs = plt.subplots(2, 5, figsize=(15, 10))\n",
    "            axs = axs.flatten()  # Flatten the array of axes to easily iterate over it\n",
    "\n",
    "            for class_label, class_activations in enumerate(activations):\n",
    "                # Convert activations to numpy array for plotting\n",
    "                class_activations = np.array(class_activations)\n",
    "\n",
    "                # Create a bar plot for the activations\n",
    "                axs[class_label].bar(range(len(class_activations)), class_activations)\n",
    "                axs[class_label].set_title(f'Activations for Class {class_label}')\n",
    "                axs[class_label].set_xlabel('Activation Index')\n",
    "                axs[class_label].set_ylabel('Activation Value')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Visualize activations for a specific class\n",
    "            print(f\"Class {class_idx}:\")\n",
    "            print(activations[class_idx])\n",
    "\n",
    "    def visualize_activations_for_neuron(self, layer_idx, neuron_idx):\n",
    "        layer_activations = self.compute_activations()\n",
    "        activations = layer_activations[layer_idx]\n",
    "\n",
    "        # Visualize the activations for the specified neuron\n",
    "        neuron_activations = [class_activations[neuron_idx] for class_activations in activations]\n",
    "        plt.bar(range(len(neuron_activations)), neuron_activations)\n",
    "        plt.title(f'Activations for Neuron {neuron_idx}')\n",
    "        plt.xlabel('Class Index')\n",
    "        plt.ylabel('Activation Value')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_activations_for_layer(self, visual_type):\n",
    "        layer_outputs = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'output'):\n",
    "                layer_model = tf.keras.Model(inputs=self.model.input, outputs=layer.output)\n",
    "                layer_outputs.append(layer_model.predict(self.x_test))\n",
    "\n",
    "        # Compute the neuron activations averaged for each label and each layer\n",
    "        layer_activations = []\n",
    "        for outputs in layer_outputs:\n",
    "            num_neurons_in_layer = outputs.shape[-1]\n",
    "            layer_activation = np.zeros((self.y_test.shape[-1], num_neurons_in_layer))\n",
    "            for label_idx in range(self.y_test.shape[-1]):\n",
    "                layer_activation[label_idx] = np.mean(outputs[np.argmax(self.y_test, axis=1) == label_idx], axis=0)\n",
    "            layer_activations.append(layer_activation)\n",
    "\n",
    "        # Visualize the activations for each layer\n",
    "        if visual_type == 1:\n",
    "            for layer in layer_activations:\n",
    "                plt.figure(figsize=(5, 12))  # Set the initial figure size\n",
    "                plt.matshow(layer, cmap='hot', interpolation='nearest', aspect=20)\n",
    "                plt.gcf().set_figheight(5)\n",
    "                plt.gcf().set_figwidth(30)\n",
    "                plt.show()\n",
    "        elif visual_type == 2:\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "            gs = gridspec.GridSpec(2, 3, height_ratios=[1, 1])\n",
    "            ax0 = plt.subplot(gs[0, :])\n",
    "            ax1 = plt.subplot(gs[1, 0])\n",
    "            ax2 = plt.subplot(gs[1, 1])\n",
    "            ax3 = plt.subplot(gs[1, 2])\n",
    "\n",
    "            axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                layer = layer_activations[i]\n",
    "                im = ax.matshow(layer, cmap='hot', interpolation='nearest', aspect=10)\n",
    "                ax.set_title(f'Layer {i+1} Activations')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, axs = plt.subplots(4, 1, figsize=(30, 60))\n",
    "\n",
    "            for i, layer in enumerate(layer_activations):\n",
    "                im = axs[i].matshow(layer, cmap='hot', interpolation='nearest', aspect=20)\n",
    "                axs[i].set_title(f'Layer {i+1} Activations')\n",
    "\n",
    "            # Add a colorbar to the figure\n",
    "            fig.colorbar(im, ax=axs.ravel().tolist(), orientation='horizontal', pad=0.02)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def analyze_activations(self, layer_idx, analysis_function):\n",
    "        layer_activations = self.compute_activations()\n",
    "        activations = layer_activations[layer_idx]\n",
    "\n",
    "        # Apply the analysis function to the activations\n",
    "        analysis_result = analysis_function(activations)\n",
    "\n",
    "        return analysis_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train, y_train, x_test, y_test = preprocess_data(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Create an instance of the MLPModel\n",
    "mlp_model = MLPModel(input_shape=(784,), num_classes=10)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.train(x_train, y_train)\n",
    "\n",
    "# Compute neuron activations\n",
    "neuron_activation_computer = NeuronActivationComputer(mlp_model.model, x_test, y_test)\n",
    "layer_activations = neuron_activation_computer.compute_activations()\n",
    "neuron_activation_computer.visualize_activations_for_layer(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going o go on a side track. Here my vision is to create a \"Phantom 4\" Where I perform data augmentation to remove the stalks from the 4s and then prove that I can sill recover the 4s with high accuracy using the ablation to elarn the stalk identification fromt the mean activation using PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
